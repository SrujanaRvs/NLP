{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DxLfDm4r9rzo",
    "outputId": "d6fd5f62-38f2-472d-f493-a6e22c0c8657"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: category_encoders in c:\\users\\venka\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn>=0.20.0 in c:\\users\\venka\\anaconda3\\lib\\site-packages (from category_encoders) (0.23.1)\n",
      "Requirement already satisfied, skipping upgrade: pandas>=0.21.1 in c:\\users\\venka\\anaconda3\\lib\\site-packages (from category_encoders) (1.0.5)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=1.0.0 in c:\\users\\venka\\anaconda3\\lib\\site-packages (from category_encoders) (1.5.2)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.14.0 in c:\\users\\venka\\anaconda3\\lib\\site-packages (from category_encoders) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: patsy>=0.5.1 in c:\\users\\venka\\anaconda3\\lib\\site-packages (from category_encoders) (0.5.1)\n",
      "Requirement already satisfied, skipping upgrade: statsmodels>=0.9.0 in c:\\users\\venka\\anaconda3\\lib\\site-packages (from category_encoders) (0.11.1)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in c:\\users\\venka\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20.0->category_encoders) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in c:\\users\\venka\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20.0->category_encoders) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in c:\\users\\venka\\anaconda3\\lib\\site-packages (from pandas>=0.21.1->category_encoders) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in c:\\users\\venka\\anaconda3\\lib\\site-packages (from pandas>=0.21.1->category_encoders) (2020.1)\n",
      "Requirement already satisfied, skipping upgrade: six in c:\\users\\venka\\anaconda3\\lib\\site-packages (from patsy>=0.5.1->category_encoders) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oW3-PlXbMVCj",
    "outputId": "b6766afa-88b6-4442-f8d0-11a0406c01d8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\venka\\anaconda3\\lib\\runpy.py:127: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\venka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!python -m nltk.downloader stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "exnSzMfDNInq",
    "outputId": "7c793243-d77d-4034-e161-021838ab3ebc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\venka\\anaconda3\\lib\\runpy.py:127: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\venka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!python -m nltk.downloader punkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "AE6nrG9CMY6L"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "HhbzRuPT9ub_"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import category_encoders \n",
    "from category_encoders.binary import BinaryEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Ll1zZizAE4Td"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-WasURJlJcKE"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Ignore the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "YWxK3jnnXoWs"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7de9U-IFaf1b"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Description': 'While removing the drill rod of the Jumbo 08 for maintenance, the supervisor proceeds to loosen the support of the intermediate centralizer to facilitate the removal, seeing this the mechanic supports one end on the drill of the equipment to pull with both hands the bar and accelerate the removal from this, at this moment the bar slides from its point of support and tightens the fingers of the mechanic between the drilling bar and the beam of the jumbo.',\n",
       " 'CriticalRisk': 'Pressed',\n",
       " 'IncidentDate': '2016-01-01  12:00:00',\n",
       " 'CountryName': 'Country_01',\n",
       " 'Location': 'Local_01',\n",
       " 'IndustrialSector': 'Mining',\n",
       " 'Gender': 'Male',\n",
       " 'Employment Type': 'Third Party',\n",
       " 'Potential Accident Level': 'IV'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#user_dict =  json.loads(open('/content/drive/MyDrive/NLP_Chatbot_Capstone_Project/user_data.json').read())\n",
    "user_dict =  json.loads(open('user_data.json').read())\n",
    "user_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "13m0lm19Z6Um"
   },
   "outputs": [],
   "source": [
    "# temp_test_entry={\n",
    "#     'Data':'2016-01-01 00:00:00',\n",
    "#     'Countries':'Country_01',\n",
    "#     'Local':'Local_01',\n",
    "#     'Industry Sector':'Mining',\n",
    "#     'Accident Level': 'I',\n",
    "#     'Potential Accident Level':'IV',\n",
    "#     'Genre':'Male',\n",
    "#     'Employee or Third Party':'Third Party',\n",
    "#     'Critical Risk':'Pressed',\n",
    "#     'Description':'While removing the drill rod of the Jumbo 08 for maintenance, the supervisor proceeds to loosen the support of the intermediate centralizer to facilitate the removal, seeing this the mechanic supports one end on the drill of the equipment to pull with both hands the bar and accelerate the removal from this, at this moment the bar slides from its point of support and tightens the fingers of the mechanic between the drilling bar and the beam of the jumbo.',\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "J3X7tac6ZZ7F"
   },
   "outputs": [],
   "source": [
    "temp_test_entry={}\n",
    "temp_test_entry['Description'] = user_dict['Description']\n",
    "temp_test_entry['Critical Risk'] = user_dict['CriticalRisk']\n",
    "temp_test_entry['Data'] = user_dict['IncidentDate']\n",
    "temp_test_entry['Countries'] = user_dict['CountryName']\n",
    "temp_test_entry['Local'] = user_dict['Location']\n",
    "temp_test_entry['Industry Sector'] = user_dict['IndustrialSector']\n",
    "temp_test_entry['Genre'] = user_dict['Gender']\n",
    "temp_test_entry['Employee or Third Party'] = user_dict['Employment Type']\n",
    "temp_test_entry['Potential Accident Level'] = user_dict['Potential Accident Level']\n",
    "#temp_test_entry['Accident Level'] = user_dict['Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "TBLitKfXZZhB"
   },
   "outputs": [],
   "source": [
    "date_event=datetime.strptime(temp_test_entry['Data'], '%Y-%m-%d %H:%M:%S')\n",
    "temp_test_entry['Date of Incidents']=date_event\n",
    "temp_test_entry['Year of Incident'],temp_test_entry['Month of Incident']=date_event.year,date_event.month\n",
    "temp_test_entry['Date of Incident'],temp_test_entry['Day of Incident']=date_event.day,calendar.day_name[date_event.weekday()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "xQ1c0X7BcXW4"
   },
   "outputs": [],
   "source": [
    "#path_initial_encoders='/content/drive/MyDrive/NLP_Chatbot_Capstone_Project/Initial Encoders'\n",
    "#path_cleaning_encoders='/content/drive/MyDrive/NLP_Chatbot_Capstone_Project/Cleaning Encoders'\n",
    "#path_tf_idf_vectorizer='/content/drive/MyDrive/NLP_Chatbot_Capstone_Project/Trained TF-IDF Vectorizer'\n",
    "#path_default_classifiers='/content/drive/MyDrive/NLP_Chatbot_Capstone_Project/Trained Default Machine Learning Classifiers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_initial_encoders='Initial Encoders'\n",
    "path_cleaning_encoders='Cleaning Encoders'\n",
    "path_tf_idf_vectorizer='Trained TF-IDF Vectorizer'\n",
    "path_default_classifiers='Trained Default Machine Learning Classifiers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "VX7jRxCs8ZOA"
   },
   "outputs": [],
   "source": [
    "encoded_test_entry=temp_test_entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Okctuid48G5z"
   },
   "source": [
    "### **Initial Encoders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kwByBLjKJcC7",
    "outputId": "ee2743a4-34e0-42c0-c31f-55d9be0aa393"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'Initial Encoders'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-158f0c46702c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfiles\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_initial_encoders\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_initial_encoders\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'Initial Encoders'"
     ]
    }
   ],
   "source": [
    "files=[f for f in listdir(path_initial_encoders) if isfile(join(path_initial_encoders,f))]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#files.remove('Ordinal Encoder_Accident Level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "NdL8c-KQALV8"
   },
   "outputs": [],
   "source": [
    "ordinal_encoded_features=[]\n",
    "label_encoded_features=[]\n",
    "binary_encoded_features=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QfUoijzWJcAF",
    "outputId": "c3228bc0-dac7-4123-a273-81ad536c1b65"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-6791659619ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m   \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_initial_encoders\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m\u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mencoder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'Ordinal Encoder_Accident Level'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mfinal_encoder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'files' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(files)):\n",
    "  with open(path_initial_encoders+'/'+files[i] ,'rb') as f:\n",
    "    encoder=pickle.load(f)\n",
    "  if files[i]=='Ordinal Encoder_Accident Level':\n",
    "    final_encoder=encoder\n",
    "  feature_name=files[i].split('_')[1].strip()\n",
    "  if files[i].split('_')[0].strip().__contains__('Label'):\n",
    "    encoded_test_entry[feature_name]=encoder.transform(np.array(temp_test_entry[feature_name]).reshape(-1,1))[0]\n",
    "    label_encoded_features.append(feature_name)\n",
    "    # print(encoder.classes_)\n",
    "  elif files[i].split('_')[0].strip().__contains__('Ordinal'):\n",
    "    if feature_name=='Accident Level':\n",
    "      continue\n",
    "    else:\n",
    "      encoded_test_entry[feature_name]=encoder.transform(np.array(temp_test_entry[feature_name]).reshape(-1,1))[0][0]\n",
    "      ordinal_encoded_features.append(feature_name)\n",
    "      # print(encoder.categories_)\n",
    "  else:\n",
    "    continue\n",
    "encoded_test_entry\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngkZ6-XR8M1C"
   },
   "source": [
    "### **Cleaning Encoders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lwHLvA1Y8OX6",
    "outputId": "860e07b4-5453-4491-ad40-2b2c70aac3d7"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'Cleaning Encoders'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-e773c50d9c1d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfiles\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_cleaning_encoders\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_cleaning_encoders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'Cleaning Encoders'"
     ]
    }
   ],
   "source": [
    "files=[f for f in listdir(path_cleaning_encoders) if isfile(join(path_cleaning_encoders, f))]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1456Og7h8OlJ",
    "outputId": "6e409220-344e-4aa2-b83c-e0608edc02d6"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-364545940e23>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m   \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m   \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_cleaning_encoders\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m\u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mencoder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m   \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'files' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(files)):\n",
    "  print(files[i])\n",
    "  with open(path_cleaning_encoders+'/'+files[i] ,'rb') as f:\n",
    "    encoder=pickle.load(f)\n",
    "  feature_name=files[i].split('_')[1].strip()\n",
    "  if files[i].split('_')[0].strip().__contains__('Label'):\n",
    "    # print(feature_name,temp_test_entry[feature_name])\n",
    "    # print(encoder.transform(np.array(temp_test_entry[feature_name]).reshape(-1,1)))    \n",
    "    encoded_test_entry[feature_name]=encoder.transform(np.array(temp_test_entry[feature_name]).reshape(-1,1))[0]\n",
    "    label_encoded_features.append(feature_name)\n",
    "    # print(encoder.classes_)\n",
    "  elif files[i].split('_')[0].strip().__contains__('Ordinal'):\n",
    "    # print(feature_name,temp_test_entry[feature_name])\n",
    "    # print(encoder.transform(np.array(temp_test_entry[feature_name]).reshape(-1,1)))\n",
    "    encoded_test_entry[feature_name]=encoder.transform(np.array(temp_test_entry[feature_name]).reshape(-1,1))[0][0]\n",
    "    ordinal_encoded_features.append(feature_name)\n",
    "    # print(encoder.categories_)\n",
    "  elif files[i].split('_')[0].strip().__contains__('Binary'):\n",
    "    binary_encoder=encoder\n",
    "  elif files[i].split('_')[0].strip().__contains__('Scaler'):\n",
    "    # print(feature_name)\n",
    "    value=(np.array([encoded_test_entry['Potential Accident Level']]).reshape(-1,1))\n",
    "    encoded_test_entry['Potential Accident Level']=encoder.transform(value)[0][0]\n",
    "  else:\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "69oWoLQA8Oan"
   },
   "outputs": [],
   "source": [
    "#y=encoded_test_entry['Accident Level']\n",
    "#del encoded_test_entry['Accident Level']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "amJecOKMHYCc",
    "outputId": "a089ac42-df7d-4714-de38-848511619250"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Description', 'Critical Risk', 'Data', 'Countries', 'Local',\n",
       "       'Industry Sector', 'Genre', 'Employee or Third Party',\n",
       "       'Potential Accident Level', 'Date of Incidents', 'Year of Incident',\n",
       "       'Month of Incident', 'Date of Incident', 'Day of Incident', 'index'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_test_entry['index']=0\n",
    "temp=[]\n",
    "for i in list(encoded_test_entry.keys()):\n",
    "  temp.append(encoded_test_entry[i])\n",
    "temp=pd.DataFrame(np.array([temp]),columns=list(encoded_test_entry.keys()))\n",
    "temp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "XgcewzXjJZT_"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'binary_encoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-9d2968b2b847>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mencodings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbinary_encoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'binary_encoder' is not defined"
     ]
    }
   ],
   "source": [
    "encodings=binary_encoder.transform(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSoMhaMwKnvW"
   },
   "source": [
    "### **TF-IDF Vectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5XVTbPBXKZAt",
    "outputId": "9b1287a4-7a8c-4114-90f8-2c5a414e1630"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'Trained TF-IDF Vectorizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-397d6f7e44cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfiles\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_tf_idf_vectorizer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_tf_idf_vectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'Trained TF-IDF Vectorizer'"
     ]
    }
   ],
   "source": [
    "files=[f for f in listdir(path_tf_idf_vectorizer) if isfile(join(path_tf_idf_vectorizer, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Yqm-FZl5RPlI"
   },
   "outputs": [],
   "source": [
    "stop_words = list(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)\n",
    "stop_words_list = stop_words+punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "suq0Vo6GMzha"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "      text = text.lower()\n",
    "      pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "      text = pattern.sub('', text)\n",
    "      text = \" \".join(filter(lambda x:x[0]!='@', text.split()))\n",
    "      emoji = re.compile(\"[\"\n",
    "                            u\"\\U0001F600-\\U0001FFFF\"  # emoticons\n",
    "                            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                            u\"\\U00002702-\\U000027B0\"\n",
    "                            u\"\\U000024C2-\\U0001F251\"\n",
    "                            \"]+\", flags=re.UNICODE)\n",
    "      \n",
    "      text = emoji.sub(r'', text)\n",
    "      text = text.lower()\n",
    "      text = re.sub(r\"i'm\", \"i am\", text)\n",
    "      text = re.sub(r\"he's\", \"he is\", text)\n",
    "      text = re.sub(r\"she's\", \"she is\", text)\n",
    "      text = re.sub(r\"that's\", \"that is\", text)        \n",
    "      text = re.sub(r\"what's\", \"what is\", text)\n",
    "      text = re.sub(r\"where's\", \"where is\", text) \n",
    "      text = re.sub(r\"\\'ll\", \" will\", text)  \n",
    "      text = re.sub(r\"\\'ve\", \" have\", text)  \n",
    "      text = re.sub(r\"\\'re\", \" are\", text)\n",
    "      text = re.sub(r\"\\'d\", \" would\", text)\n",
    "      text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "      text = re.sub(r\"won't\", \"will not\", text)\n",
    "      text = re.sub(r\"don't\", \"do not\", text)\n",
    "      text = re.sub(r\"did't\", \"did not\", text)\n",
    "      text = re.sub(r\"can't\", \"can not\", text)\n",
    "      text = re.sub(r\"it's\", \"it is\", text)\n",
    "      text = re.sub(r\"couldn't\", \"could not\", text)\n",
    "      text = re.sub(r\"have't\", \"have not\", text)\n",
    "      text = re.sub(r\"[,.\\\"\\'!@#$%^&*(){}?/;`~:<>+=-]\", \"\", text)\n",
    "      return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "uonmzBzTPM6t"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encodings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-8b8300c359eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mencodings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Merged_Description'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencodings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Critical Risk'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mencodings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Description'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'encodings' is not defined"
     ]
    }
   ],
   "source": [
    "encodings['Merged_Description']=encodings['Critical Risk']+encodings['Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "ksU5c8KbKY9p"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-0cc42bc9fa98>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_tf_idf_vectorizer\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m   \u001b[0mtf_idf_vec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mencodings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Clean Words'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencodings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Description'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstop_words_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mwords_transformed_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf_idf_vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencodings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Clean Words'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mencoded_bag_of_words_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords_transformed_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'word_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mtf_idf_vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwords_transformed_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'files' is not defined"
     ]
    }
   ],
   "source": [
    "with open(path_tf_idf_vectorizer+'/'+files[0],'rb') as f:\n",
    "  tf_idf_vec=pickle.load(f)\n",
    "encodings['Clean Words']=(clean_text(\" \".join([w for w in word_tokenize(encodings['Description'][0]) if not w in stop_words_list])))\n",
    "words_transformed_test=tf_idf_vec.transform(encodings['Clean Words'])\n",
    "encoded_bag_of_words_model=pd.DataFrame(words_transformed_test.todense(),columns=['word_'+str(i)+'_'+tf_idf_vec.get_feature_names()[i] for i in range(0,words_transformed_test.todense().shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "vX7nIQIrKY6a"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encodings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-7a24a8b5d159>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfinal_dataframe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mencodings\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoded_bag_of_words_model\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'encodings' is not defined"
     ]
    }
   ],
   "source": [
    "final_dataframe=pd.concat([encodings,encoded_bag_of_words_model],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "r94e1R8mPAQV"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_dataframe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-f5dfa554c67f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcolumns_to_delete\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Critical Risk'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Description'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Data'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Date of Incidents'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Merged_Description'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Clean Words'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcolumns_to_delete\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m   \u001b[1;32mdel\u001b[0m \u001b[0mfinal_dataframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'final_dataframe' is not defined"
     ]
    }
   ],
   "source": [
    "columns_to_delete=['Critical Risk','Description','Data','Date of Incidents','Merged_Description','index','Clean Words']\n",
    "for j in columns_to_delete:\n",
    "  del final_dataframe[j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IKknfwz4Pi90"
   },
   "source": [
    "### **Modelling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "_6FradYVPraP",
    "outputId": "96cc6ec3-226f-4214-9c44-776effb430c3"
   },
   "outputs": [],
   "source": [
    "files=[f for f in listdir(path_default_classifiers) if isfile(join(path_default_classifiers, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "EW4CJE3QPj_E"
   },
   "outputs": [],
   "source": [
    "with open(path_default_classifiers+'/'+files[0],'rb') as f:\n",
    "  random_forest=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "kTrZjmqOPj8m"
   },
   "outputs": [],
   "source": [
    "prediction=random_forest.predict(final_dataframe.iloc[0].values.reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kcEpqv49Pj5h",
    "outputId": "3b39774c-a3e0-4449-9501-2473762a12e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_class=final_encoder.inverse_transform([prediction])[0][0]\n",
    "#predicted_class,final_encoder.inverse_transform([[y]])[0][0]\n",
    "predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "name": "NLP_Capstone_Server_Loading_File.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
